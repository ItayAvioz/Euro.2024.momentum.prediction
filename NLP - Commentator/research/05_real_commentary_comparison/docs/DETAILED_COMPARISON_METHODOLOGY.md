# Detailed Commentary Comparison Methodology

## Overview

This document provides an in-depth explanation of the methods, models, and parameters used to compare our AI-generated football commentary with real Sports Mole commentary for Euro 2024 matches.

---

## Comparison Strategy

### Minute-Level Comparison
- **Goal**: For each minute of the match, compare ALL generated sequence commentaries to the real commentary for that same minute
- **Output**: Ranked list of sequences per minute (1 = most similar, descending order)
- **Metrics Used**: Three complementary similarity metrics

### Data Normalization
To ensure fair comparison, we normalize:
1. **Team Names**: "England" ‚Üî "Three Lions" ‚Üî "The English"
2. **Player Names**: "Lamine Yamal Nasraoui Ebana" ‚Üî "Yamal" ‚Üî "Lamine Yamal"
3. **Text Cleaning**: Remove emojis, special characters, extra whitespace

---

## Method 1: Cosine Similarity (TF-IDF)

### What It Measures
**Lexical similarity** - How similar are the words and phrases used in both commentaries?

### How It Works

#### Step 1: Text Vectorization (TF-IDF)
TF-IDF = **Term Frequency-Inverse Document Frequency**

```python
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(
    lowercase=True,           # Convert to lowercase
    stop_words='english',     # Remove common words (the, a, is, etc.)
    ngram_range=(1, 2),       # Use 1-word and 2-word phrases
    max_features=1000         # Limit to top 1000 terms
)
```

**Example:**
```
Text: "Palmer smashes one past Simon from the edge of the box"

After TF-IDF:
- "palmer" ‚Üí 0.45
- "smashes" ‚Üí 0.38
- "simon" ‚Üí 0.45
- "edge box" ‚Üí 0.33  (2-word phrase)
- "past" ‚Üí 0.12
```

#### Step 2: Cosine Similarity Calculation
Measures the angle between two vectors in multi-dimensional space.

```python
from sklearn.metrics.pairwise import cosine_similarity

# Convert texts to vectors
vectors = vectorizer.fit_transform([our_text, real_text])

# Calculate cosine similarity
similarity = cosine_similarity(vectors[0:1], vectors[1:2])[0][0]
```

**Mathematical Formula:**
```
cosine_similarity = (A ¬∑ B) / (||A|| √ó ||B||)

Where:
- A ¬∑ B = dot product of vectors A and B
- ||A|| = magnitude of vector A
- ||B|| = magnitude of vector B
```

**Score Range**: 0.0 to 1.0
- **1.0** = Identical word usage
- **0.7-0.9** = Very similar vocabulary
- **0.4-0.6** = Moderate similarity
- **0.0-0.3** = Very different word choice

### Example Comparison

**Our Commentary:**
> "Cole Palmer shoots from the edge of the box and scores! The ball flies past the goalkeeper into the bottom corner. Spain 1-1 England."

**Real Commentary:**
> "Palmer smashes one past Simon from the edge of the area to level the scores! All square in Berlin."

**TF-IDF Analysis:**
- Shared terms: "palmer", "edge", "box/area", "past", "scores/level"
- Our unique: "shoots", "flies", "goalkeeper", "bottom corner"
- Real unique: "smashes", "simon", "square", "berlin"

**Cosine Similarity**: ~0.62 (Good match - captures the same action)

---

## Method 2: Entity Overlap

### What It Measures
**Factual accuracy** - Are the same players, actions, locations, and outcomes mentioned?

### How It Works

#### Step 1: Entity Extraction

We extract 5 key entity types:

##### 1. Players (Named Entity Recognition)
```python
import spacy
nlp = spacy.load('en_core_web_sm')

doc = nlp(text)
players = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']
```

**Example:**
```
Text: "Palmer receives from Bellingham and shoots"
Players Extracted: ["Palmer", "Bellingham"]
```

##### 2. Actions (Verb Lemmatization)
```python
actions = [token.lemma_ for token in doc 
           if token.pos_ == 'VERB' and token.lemma_ not in ['be', 'have', 'do']]
```

**Example:**
```
Text: "Yamal receives the ball, dribbles past Walker, and shoots"
Actions Extracted: ["receive", "dribble", "shoot"]
```

##### 3. Locations (Pattern Matching)
```python
locations = []
for chunk in doc.noun_chunks:
    if any(loc_word in chunk.text.lower() 
           for loc_word in ['box', 'corner', 'wing', 'third', 'area', 'midfield']):
        locations.append(chunk.text)
```

**Example:**
```
Text: "A cross from the right wing into the penalty area"
Locations Extracted: ["right wing", "penalty area"]
```

##### 4. Outcomes (Keyword Detection)
```python
def extract_outcome(text):
    text_upper = text.upper()
    if 'GOAL' in text_upper or 'SCORES' in text_upper:
        return 'GOAL'
    elif 'SAVED' in text_upper or 'SAVE' in text_upper:
        return 'SAVED'
    elif 'MISS' in text_upper or 'OVER' in text_upper or 'WIDE' in text_upper:
        return 'MISS'
    elif 'BLOCK' in text_upper:
        return 'BLOCK'
    return 'ATTEMPT'
```

##### 5. Score (Regex Pattern)
```python
import re

def extract_score(text):
    # Pattern: "Spain 2-1 England" or "2-1" or "level"
    score_pattern = r'\b(\d+)[:-](\d+)\b'
    match = re.search(score_pattern, text)
    if match:
        return f"{match.group(1)}-{match.group(2)}"
    elif 'level' in text.lower() or 'square' in text.lower():
        return 'LEVEL'
    return None
```

#### Step 2: Calculate Overlap
```python
def calculate_entity_overlap(our_entities, real_entities):
    overlaps = {
        'players': len(set(our_entities['players']) & set(real_entities['players'])) / 
                   max(len(set(our_entities['players'] + real_entities['players'])), 1),
        
        'actions': len(set(our_entities['actions']) & set(real_entities['actions'])) / 
                   max(len(set(our_entities['actions'] + real_entities['actions'])), 1),
        
        'locations': any(our_loc in real_loc or real_loc in our_loc 
                        for our_loc in our_entities['locations'] 
                        for real_loc in real_entities['locations']),
        
        'outcome': our_entities['outcome'] == real_entities['outcome'],
        
        'score': our_entities['score'] == real_entities['score']
    }
    
    # Calculate weighted average
    weights = {'players': 0.3, 'actions': 0.25, 'locations': 0.15, 
               'outcome': 0.2, 'score': 0.1}
    
    overlap_score = sum(overlap * weights[key] 
                       for key, overlap in overlaps.items() 
                       if isinstance(overlap, (int, float)))
    
    return overlap_score, overlaps
```

### Example Comparison

**Our Commentary:**
> "Nico Williams receives from Lamine Yamal in the left attacking third and shoots with the right foot - GOAL! Spain 1-0 England."

**Real Commentary:**
> "Williams makes the breakthrough early in the second period as Yamal sets up Williams, who places one into the bottom corner. Super finish!"

**Entity Extraction:**

| Entity Type | Our Commentary | Real Commentary | Match? |
|------------|----------------|-----------------|--------|
| Players | ["Nico Williams", "Lamine Yamal"] | ["Williams", "Yamal"] | ‚úÖ 100% |
| Actions | ["receive", "shoot"] | ["set up", "place"] | ‚ö†Ô∏è 25% |
| Locations | ["left attacking third"] | ["bottom corner"] | ‚ö†Ô∏è 50% |
| Outcome | GOAL | GOAL (implied) | ‚úÖ 100% |
| Score | "1-0" | None | ‚ö†Ô∏è 0% |

**Entity Overlap Score**: 0.73 (Good match - same players and outcome)

---

## Method 3: Semantic Similarity (Sentence Transformers)

### What It Measures
**Meaning similarity** - Do both commentaries convey the same semantic meaning, even if worded differently?

### How It Works

#### Step 1: Sentence Embeddings

Uses a pre-trained neural network model: **`all-MiniLM-L6-v2`**

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')
```

**Model Details:**
- **Architecture**: 6-layer MiniLM (Distilled BERT)
- **Training**: 1 billion+ sentence pairs
- **Output**: 384-dimensional dense vector (embedding)
- **Speed**: ~3000 sentences/second on CPU

**What are embeddings?**
A numerical representation that captures semantic meaning:

```
Sentence: "Palmer scores from outside the box"
Embedding: [0.23, -0.45, 0.89, 0.12, -0.67, ..., 0.34]  (384 numbers)
           ‚îî‚îÄ These numbers encode the MEANING of the sentence
```

**Example - Similar Meanings:**
```python
sentences = [
    "Palmer shoots and scores",
    "Palmer finds the back of the net",
    "Goal by Palmer!"
]

embeddings = model.encode(sentences)

# All three embeddings will be VERY CLOSE in 384D space
# because they mean the same thing!
```

#### Step 2: Cosine Distance Calculation

```python
from scipy.spatial.distance import cosine

our_embedding = model.encode(our_text)
real_embedding = model.encode(real_text)

# Calculate semantic similarity
semantic_similarity = 1 - cosine(our_embedding, real_embedding)
```

**Score Range**: -1.0 to 1.0 (typically 0.0 to 1.0 for similar texts)
- **0.9-1.0** = Nearly identical meaning
- **0.7-0.8** = Very similar meaning
- **0.5-0.6** = Related topics
- **0.0-0.4** = Different meanings

### Example Comparison

**Our Commentary:**
> "Cole Palmer receives the ball from Jude Bellingham at the edge of the penalty area and fires a shot with his right foot into the bottom left corner. The goalkeeper has no chance! Spain 1-1 England."

**Real Commentary:**
> "Palmer smashes one past Simon from the edge of the area to level the scores! Chelsea's man makes an instant impact!"

**Semantic Analysis:**

Both commentaries express:
- ‚úÖ Palmer scored
- ‚úÖ From edge of box/area
- ‚úÖ Leveled the score
- ‚úÖ Important goal

**Semantic Similarity**: ~0.78 (High - same core meaning despite different wording)

---

## Ranking Logic

### Per-Minute Ranking

For each minute, we rank ALL generated sequences against real commentary:

```python
def rank_sequences_in_minute(minute, our_sequences, real_commentary):
    results = []
    
    for seq in our_sequences:
        # Calculate all three metrics
        cosine_sim = calculate_cosine_similarity(seq, real_commentary)
        entity_overlap = calculate_entity_overlap(seq, real_commentary)
        semantic_sim = calculate_semantic_similarity(seq, real_commentary)
        
        # Store all scores
        results.append({
            'sequence': seq,
            'cosine_similarity': cosine_sim,
            'entity_overlap': entity_overlap,
            'semantic_similarity': semantic_sim,
            'average_score': (cosine_sim + entity_overlap + semantic_sim) / 3
        })
    
    # Sort by average score (highest first)
    results.sort(key=lambda x: x['average_score'], reverse=True)
    
    # Assign ranks
    for i, result in enumerate(results):
        result['rank'] = i + 1
    
    return results
```

### Key Sequence Identification

Rank 1 sequences are considered **"key sequences"** - those that best match the real commentary style and content.

**Characteristics of Rank 1 Sequences:**
- Mention same players as real commentary
- Capture same action/event
- Similar excitement level
- Accurate factual details

---

## Normalization Strategy

### Team Name Normalization

```python
TEAM_VARIATIONS = {
    'spain': ['spain', 'spanish', 'la roja', 'spaniards'],
    'england': ['england', 'english', 'three lions', 'the three lions'],
    'france': ['france', 'french', 'les bleus'],
    'netherlands': ['netherlands', 'dutch', 'holland', 'oranje']
}

def normalize_team_name(text):
    text_lower = text.lower()
    for canonical, variations in TEAM_VARIATIONS.items():
        for variation in variations:
            text_lower = text_lower.replace(variation, canonical)
    return text_lower
```

### Player Name Normalization

```python
PLAYER_VARIATIONS = {
    'yamal': ['lamine yamal', 'yamal', 'lamine yamal nasraoui ebana'],
    'williams': ['nico williams', 'williams', 'nico'],
    'palmer': ['cole palmer', 'palmer', 'cole'],
    'bellingham': ['jude bellingham', 'bellingham', 'jude'],
    # ... etc
}

def normalize_player_names(text):
    text_lower = text.lower()
    for canonical, variations in PLAYER_VARIATIONS.items():
        for variation in variations:
            text_lower = text_lower.replace(variation, canonical)
    return text_lower
```

---

## Output Format

### Per-Match Results CSV

Each match gets a CSV: `match_[id]_comparison_results.csv`

**Columns:**
1. `minute` - Match minute
2. `real_commentary` - Sports Mole commentary
3. `real_commentary_type` - "red" (key event) or "black" (general)
4. `sequence_rank` - Rank within that minute (1 = best match)
5. `our_sequence_commentary` - Generated commentary
6. `our_sequence_id` - Sequence identifier
7. `cosine_similarity` - Score (0-1)
8. `entity_overlap` - Score (0-1)
9. `semantic_similarity` - Score (0-1)
10. `average_score` - Mean of three metrics
11. `score_before` - Match score before events

### Summary Statistics CSV

`comparison_summary_statistics.csv`

**Metrics:**
- Average scores per metric per match
- Distribution of Rank 1 sequences by event type
- Coverage: % of real commentary minutes with matching generated sequences
- Best/worst performing minutes

---

## Example: Full Minute Analysis

### Minute 73 - Spain vs England Final

**Real Commentary (Sports Mole):**
> "All square in the final! Palmer smashes one past Simon from the edge of the box to level the scores! Saka was down the right, Bellingham then set up Palmer, and the Chelsea attacker managed to find the back of the net to level it up in Berlin."

**Our Generated Sequences in Minute 73:**

#### Rank 1 (Average Score: 0.79)
**Sequence Commentary:**
> "Bukayo Saka receives the ball in the right attacking third and plays a short pass along the ground to Jude Bellingham. Bellingham under pressure, plays a short pass along the ground to Cole Palmer. ‚öΩ GOAL! Cole Palmer shoots with the right foot from the central attacking third - IT'S A GOAL! What a strike! üéØ Spain 1-1 England (Palmer's 1st goal of the tournament)"

**Scores:**
- Cosine Similarity: 0.72
- Entity Overlap: 0.85 (Players: ‚úÖ Saka, Bellingham, Palmer | Outcome: ‚úÖ GOAL | Score: ‚úÖ 1-1)
- Semantic Similarity: 0.81

**Why Rank 1?**
- ‚úÖ Mentions all three players (Saka ‚Üí Bellingham ‚Üí Palmer)
- ‚úÖ Captures goal outcome
- ‚úÖ Correct score (1-1)
- ‚úÖ Location match (edge of box ‚âà central attacking third)

#### Rank 2 (Average Score: 0.54)
**Sequence Commentary:**
> "Marc Guehi clears the ball from the central defensive third with a header. Harry Kane receives the ball and plays a long pass through the air to Bukayo Saka."

**Scores:**
- Cosine Similarity: 0.38
- Entity Overlap: 0.45 (Players: ‚ö†Ô∏è Kane, Saka - but not the goal players)
- Semantic Similarity: 0.79

**Why Rank 2?**
- ‚ö†Ô∏è Different event (clearance + pass, not goal)
- ‚ö†Ô∏è Wrong players (Guehi, Kane vs Palmer)
- ‚úÖ Same minute context

---

## Model Performance Expectations

### Typical Score Ranges

| Metric | Excellent | Good | Fair | Poor |
|--------|-----------|------|------|------|
| Cosine Similarity | 0.7+ | 0.5-0.7 | 0.3-0.5 | <0.3 |
| Entity Overlap | 0.8+ | 0.6-0.8 | 0.4-0.6 | <0.4 |
| Semantic Similarity | 0.8+ | 0.6-0.8 | 0.4-0.6 | <0.4 |

### Expected Challenges

1. **Style Mismatch**: Our commentary is more detailed and structured
2. **Timing Granularity**: We have event-level precision, real commentary aggregates multiple events
3. **Excitement Level**: Real commentators have retrospective knowledge and adjust tone
4. **Name Variations**: "Lamine Yamal" vs "Yamal" vs "the teenager"

---

## Files Generated

### Scripts
1. `compare_commentary.py` - Main comparison script
2. `normalize_text.py` - Text normalization utilities

### Data Outputs
1. `match_3943043_comparison_results.csv` - Spain vs England Final
2. `match_3942819_comparison_results.csv` - Netherlands vs England Semi
3. `match_3942752_comparison_results.csv` - Spain vs France Semi
4. `comparison_summary_statistics.csv` - Overall metrics

### Documentation
1. `DETAILED_COMPARISON_METHODOLOGY.md` - This document
2. `COMPARISON_RESULTS_ANALYSIS.md` - Findings and insights (generated after comparison)

---

## Next Steps

1. ‚úÖ Implement comparison script with all three metrics
2. ‚úÖ Process all three matches
3. ‚úÖ Generate ranked results CSVs
4. ‚è≥ Analyze results and identify patterns
5. ‚è≥ Refine commentary generation based on findings

---

**Document Version**: 1.0  
**Last Updated**: October 24, 2025  
**Author**: AI Commentary Generation Project

